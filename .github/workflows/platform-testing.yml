name: Scraper Platform Testing and Quality Assurance

# This workflow automates testing of scraper functionality in a local environment
# and performs data quality checks to ensure scraper reliability and data integrity.
# It runs on pushes to scraper-related code, daily schedules, and manual triggers.

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/scrapers/**'
      - 'src/scrapers/config/**'
      - 'src/scrapers/executor/**'
      - 'src/scrapers/models/**'
      - 'src/scrapers/parser/**'
      - 'src/scrapers/schemas/**'
      - '.github/workflows/platform-testing.yml'
    # Triggers on changes to scraper-related files in main/develop branches
  schedule:
    # Run daily at 2 AM UTC to ensure ongoing scraper health
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      scrapers:
        description: 'Comma-separated list of scrapers to test (leave empty to test all scrapers)'
        required: false
        default: ''

jobs:
  # Job to run automated tests on scrapers in a local environment
  local-test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH

    - name: Cache uv dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/uv
        key: ${{ runner.os }}-uv-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-uv-

    - name: Create virtual environment
      run: |
        uv venv

    - name: Install dependencies
      run: |
        uv pip install -e .

    - name: Determine scrapers to test
      id: scrapers
      run: |
        if [ -n "${{ github.event.inputs.scrapers }}" ]; then
          echo "scrapers=${{ github.event.inputs.scrapers }}" >> $GITHUB_OUTPUT
        else
          echo "scrapers=all" >> $GITHUB_OUTPUT
        fi

    - name: Run local tests
      if: steps.scrapers.outputs.scrapers == 'all'
      run: |
        uv run python tests/platform_test_scrapers.py --all --local --verbose
      env:
        PYTHONPATH: ${{ github.workspace }}

    - name: Run specific scraper tests
      if: steps.scrapers.outputs.scrapers != 'all'
      run: |
        for scraper in $(echo "${{ github.event.inputs.scrapers }}" | tr ',' '\n'); do
          echo "Testing scraper: $scraper"
          uv run python tests/platform_test_scrapers.py --scraper "$scraper" --local --verbose
        done
      env:
        PYTHONPATH: ${{ github.workspace }}

    - name: Generate test report
      if: always()
      run: |
        echo "## Local Testing Report" > test_report.md
        echo "- **Date:** $(date)" >> test_report.md
        echo "- **Mode:** local" >> test_report.md
        echo "- **Scrapers:** ${{ steps.scrapers.outputs.scrapers }}" >> test_report.md
        echo "- **Status:** ${{ job.status }}" >> test_report.md
        echo "" >> test_report.md
        echo "### Test Results" >> test_report.md
        echo "\`\`\`" >> test_report.md
        cat test_output.log >> test_report.md 2>/dev/null || echo "No test output available" >> test_report.md
        echo "\`\`\`" >> test_report.md

    - name: Upload test report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: local-test-report
        path: test_report.md

    - name: Notify on failure
      if: failure()
      run: |
        echo "âŒ Local testing failed!"
        # Add notification logic here (Slack, email, etc.)

  # Job to analyze data quality of scraper outputs and archive results
  data-quality-check:
    needs: local-test
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH

    - name: Create virtual environment
      run: |
        uv venv

    - name: Install dependencies
      run: |
        uv pip install -e .

    - name: Run data quality analysis
      run: |
        uv run python -c "
        from tests.fixtures.scraper_validator import ScraperValidator
        import json

        # Load test data
        with open('tests/fixtures/scraper_test_data.json', 'r') as f:
            test_data = json.load(f)

        validator = ScraperValidator('tests/fixtures/scraper_test_data.json')

        print('ðŸ” Data Quality Analysis')
        print('=' * 50)

        for scraper_name in test_data.keys():
            if scraper_name == '_metadata':
                continue

            print(f'\\nðŸ“Š {scraper_name.upper()}')
            print('-' * 30)

            # This would analyze recent test results
            # For now, just show configuration
            config = test_data[scraper_name]
            skus = config.get('test_skus', [])
            print(f'Test SKUs: {len(skus)} configured')
            print(f'Expected Fields: {len(config.get(\"expected_fields\", []))}')

        print('\\nâœ… Data quality check completed')
        "

    - name: Archive test results
      run: |
        mkdir -p test-results-archive
        cp -r results/* test-results-archive/ 2>/dev/null || true
        echo "Test results archived on $(date)" > test-results-archive/archive_info.txt

    - name: Upload test results archive
      uses: actions/upload-artifact@v4
      with:
        name: test-results-archive
        path: test-results-archive/