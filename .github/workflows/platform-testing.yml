name: Platform Testing

on:
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      scrapers:
        description: 'Comma-separated list of scrapers to test (empty for all)'
        required: false
        default: ''
      mode:
        description: 'Testing mode'
        required: true
        default: 'platform'
        type: choice
        options:
        - platform
        - local

jobs:
  platform-test:
    runs-on: ubuntu-latest
    environment: testing

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Configure Apify (Platform mode)
      if: github.event.inputs.mode == 'platform' || github.event.schedule == '0 2 * * *'
      run: |
        echo "APIFY_API_TOKEN=${{ secrets.APIFY_API_TOKEN }}" >> $GITHUB_ENV
        echo "APIFY_BASE_URL=https://api.apify.com/v2" >> $GITHUB_ENV

    - name: Determine scrapers to test
      id: scrapers
      run: |
        if [ -n "${{ github.event.inputs.scrapers }}" ]; then
          echo "scrapers=${{ github.event.inputs.scrapers }}" >> $GITHUB_OUTPUT
        else
          echo "scrapers=all" >> $GITHUB_OUTPUT
        fi

    - name: Run platform tests
      if: steps.scrapers.outputs.scrapers == 'all'
      run: |
        python platform_test_scrapers.py --all --${{ github.event.inputs.mode || 'platform' }} --verbose
      env:
        PYTHONPATH: ${{ github.workspace }}

    - name: Run specific scraper tests
      if: steps.scrapers.outputs.scrapers != 'all'
      run: |
        for scraper in $(echo "${{ github.event.inputs.scrapers }}" | tr ',' '\n'); do
          echo "Testing scraper: $scraper"
          python platform_test_scrapers.py --scraper "$scraper" --${{ github.event.inputs.mode || 'platform' }} --verbose
        done
      env:
        PYTHONPATH: ${{ github.workspace }}

    - name: Generate test report
      if: always()
      run: |
        echo "## Platform Testing Report" > test_report.md
        echo "- **Date:** $(date)" >> test_report.md
        echo "- **Mode:** ${{ github.event.inputs.mode || 'platform' }}" >> test_report.md
        echo "- **Scrapers:** ${{ steps.scrapers.outputs.scrapers }}" >> test_report.md
        echo "- **Status:** ${{ job.status }}" >> test_report.md
        echo "" >> test_report.md
        echo "### Test Results" >> test_report.md
        echo "\`\`\`" >> test_report.md
        cat test_output.log >> test_report.md 2>/dev/null || echo "No test output available" >> test_report.md
        echo "\`\`\`" >> test_report.md

    - name: Upload test report
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: platform-test-report
        path: test_report.md

    - name: Notify on failure
      if: failure()
      run: |
        echo "âŒ Platform testing failed!"
        # Add notification logic here (Slack, email, etc.)

  data-quality-check:
    needs: platform-test
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run data quality analysis
      run: |
        python -c "
        from tests.fixtures.scraper_validator import ScraperValidator
        import json

        # Load test data
        with open('tests/fixtures/scraper_test_data.json', 'r') as f:
            test_data = json.load(f)

        validator = ScraperValidator('tests/fixtures/scraper_test_data.json')

        print('ðŸ” Data Quality Analysis')
        print('=' * 50)

        for scraper_name in test_data.keys():
            if scraper_name == '_metadata':
                continue

            print(f'\\nðŸ“Š {scraper_name.upper()}')
            print('-' * 30)

            # This would analyze recent test results
            # For now, just show configuration
            config = test_data[scraper_name]
            skus = config.get('test_skus', [])
            print(f'Test SKUs: {len(skus)} configured')
            print(f'Expected Fields: {len(config.get(\"expected_fields\", []))}')

        print('\\nâœ… Data quality check completed')
        "

    - name: Archive test results
      run: |
        mkdir -p test-results-archive
        cp -r results/* test-results-archive/ 2>/dev/null || true
        echo "Test results archived on $(date)" > test-results-archive/archive_info.txt

    - name: Upload test results archive
      uses: actions/upload-artifact@v3
      with:
        name: test-results-archive
        path: test-results-archive/