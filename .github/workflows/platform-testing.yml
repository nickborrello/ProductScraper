name: Scraper Integration Testing and Quality Assurance

# This workflow automates pytest-based testing of scraper functionality in a local environment
# and performs data quality checks to ensure scraper reliability and data integrity.
# It runs on pushes to scraper-related code, daily schedules, and manual triggers.
#
# For push events, it intelligently tests only the scrapers whose configurations
# have been modified, improving CI/CD efficiency for incremental changes.

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/scrapers/**'
      - 'src/scrapers/configs/**'
      - 'src/scrapers/executor/**'
      - 'src/scrapers/models/**'
      - 'src/scrapers/parser/**'
      - 'src/scrapers/schemas/**'
      - 'tests/fixtures/scraper_validator.py'
      - 'tests/integration/test_scraper_integration.py'
      - 'tests/integration/test_data_quality.py'
      - 'src/core/scraper_testing_integration.py'
      - '.github/workflows/platform-testing.yml'
    # Triggers on changes to scraper-related files in main/develop branches
  schedule:
    # Run daily at 2 AM UTC to ensure ongoing scraper health
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      scrapers:
        description: 'Comma-separated list of scrapers to test (leave empty for automatic detection of changed configs)'
        required: false
        default: ''

jobs:
  # Job to run automated tests on scrapers in a local environment
  local-test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install Chrome
      run: |
        sudo apt-get update
        sudo apt-get install -y wget gnupg ca-certificates fonts-liberation libappindicator3-1 libasound2t64 libatk-bridge2.0-0 libatk1.0-0 libatspi2.0-0 libcups2 libdbus-1-3 libdrm2 libgdk-pixbuf2.0-0 libgtk-3-0 libnspr4 libnss3 libx11-xcb1 libxcomposite1 libxdamage1 libxrandr2 xdg-utils
        wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list'
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable

    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH

    - name: Cache uv dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/uv
        key: ${{ runner.os }}-uv-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-uv-

    - name: Create virtual environment
      run: |
        uv venv

    - name: Install dependencies
      run: |
        uv pip install -e .

    - name: Determine scrapers to test
      id: scrapers
      run: |
        if [ -n "${{ github.event.inputs.scrapers }}" ]; then
          echo "scrapers=${{ github.event.inputs.scrapers }}" >> $GITHUB_OUTPUT
        else
          # Check for changed scraper config files
          CHANGED_CONFIGS=$(git diff --name-only HEAD~1 HEAD -- "src/scrapers/configs/*.yaml" | sed 's|src/scrapers/configs/||' | sed 's|\.yaml||' | tr '\n' ',')
          
          if [ -n "$CHANGED_CONFIGS" ]; then
            # Remove trailing comma
            CHANGED_CONFIGS=$(echo "$CHANGED_CONFIGS" | sed 's/,$//')
            echo "scrapers=$CHANGED_CONFIGS" >> $GITHUB_OUTPUT
            echo "Testing changed scrapers: $CHANGED_CONFIGS"
          else
            echo "scrapers=all" >> $GITHUB_OUTPUT
            echo "No scraper configs changed, testing all scrapers"
          fi
        fi

    - name: Run all scraper integration tests
      if: steps.scrapers.outputs.scrapers == 'all'
      run: |
        uv run pytest tests/integration/test_scraper_integration.py::TestScraperIntegration::test_all_scrapers_integration -v --tb=short
      env:
        PYTHONPATH: ${{ github.workspace }}
        SCRAPER_HEADLESS: true
        CI: true

    - name: Run specific scraper integration tests
      if: steps.scrapers.outputs.scrapers != 'all'
      run: |
        for scraper in $(echo "${{ steps.scrapers.outputs.scrapers }}" | tr ',' '\n'); do
          echo "Testing scraper: $scraper"
          uv run pytest tests/integration/test_scraper_integration.py::TestScraperIntegration::test_scraper_execution_parametrized -k "$scraper" -v --tb=short
        done
      env:
        PYTHONPATH: ${{ github.workspace }}
        SCRAPER_HEADLESS: true
        CI: true

    - name: Generate test report
      if: always()
      run: |
        echo "## Local Testing Report" > test_report.md
        echo "- **Date:** $(date)" >> test_report.md
        echo "- **Mode:** local" >> test_report.md
        echo "- **Scrapers:** ${{ steps.scrapers.outputs.scrapers }}" >> test_report.md
        echo "- **Status:** ${{ job.status }}" >> test_report.md
        echo "" >> test_report.md
        echo "### Test Results" >> test_report.md
        echo "\`\`\`" >> test_report.md
        cat test_output.log >> test_report.md 2>/dev/null || echo "No test output available" >> test_report.md
        echo "\`\`\`" >> test_report.md

    - name: Upload test report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: local-test-report
        path: test_report.md

    - name: Notify on failure
      if: failure()
      run: |
        echo "âŒ Local testing failed!"
        # Add notification logic here (Slack, email, etc.)

  # Job to analyze data quality of scraper outputs and archive results
  data-quality-check:
    needs: local-test
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install Chrome
      run: |
        sudo apt-get update
        sudo apt-get install -y wget gnupg ca-certificates fonts-liberation libappindicator3-1 libasound2t64 libatk-bridge2.0-0 libatk1.0-0 libatspi2.0-0 libcups2 libdbus-1-3 libdrm2 libgdk-pixbuf2.0-0 libgtk-3-0 libnspr4 libnss3 libx11-xcb1 libxcomposite1 libxdamage1 libxrandr2 xdg-utils
        wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list'
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable

    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH

    - name: Create virtual environment
      run: |
        uv venv

    - name: Install dependencies
      run: |
        uv pip install -e .

    - name: Run data quality analysis
      run: |
        uv run pytest tests/integration/test_data_quality.py -v --tb=short

    - name: Archive test results
      run: |
        mkdir -p test-results-archive
        cp -r results/* test-results-archive/ 2>/dev/null || true
        echo "Test results archived on $(date)" > test-results-archive/archive_info.txt

    - name: Upload test results archive
      uses: actions/upload-artifact@v4
      with:
        name: test-results-archive
        path: test-results-archive/