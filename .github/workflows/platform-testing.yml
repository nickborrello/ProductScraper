name: Scraper Testing and Quality Assurance

# This workflow automates testing of scraper functionality in a local environment
# and performs data quality checks to ensure scraper reliability and data integrity.
# It runs on pushes to scraper-related code, daily schedules, and manual triggers.
# 
# For push events, it intelligently tests only the scrapers whose configurations
# have been modified, improving CI/CD efficiency for incremental changes.

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/scrapers/**'
      - 'src/scrapers/configs/**'
      - 'src/scrapers/executor/**'
      - 'src/scrapers/models/**'
      - 'src/scrapers/parser/**'
      - 'src/scrapers/schemas/**'
      - 'tests/fixtures/scraper_validator.py'
      - 'tests/integration/test_scraper_integration.py'
      - 'tests/integration/test_data_quality.py'
      - 'src/core/scraper_testing_integration.py'
      - '.github/workflows/platform-testing.yml'
    # Triggers on changes to scraper-related files in main/develop branches
  schedule:
    # Run daily at 2 AM UTC to ensure ongoing scraper health
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      scrapers:
        description: 'Comma-separated list of scrapers to test (leave empty for automatic detection of changed configs)'
        required: false
        default: ''

jobs:
  # Job to run automated tests on scrapers in a local environment
  local-test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install Chrome
      run: |
        sudo apt-get update
        sudo apt-get install -y wget gnupg ca-certificates fonts-liberation libappindicator3-1 libasound2 libatk-bridge2.0-0 libatk1.0-0 libatspi2.0-0 libcups2 libdbus-1-3 libdrm2 libgdk-pixbuf2.0-0 libgtk-3-0 libnspr4 libnss3 libx11-xcb1 libxcomposite1 libxdamage1 libxrandr2 xdg-utils
        wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list'
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable

    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH

    - name: Cache uv dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/uv
        key: ${{ runner.os }}-uv-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-uv-

    - name: Create virtual environment
      run: |
        uv venv

    - name: Install dependencies
      run: |
        uv pip install -e .

    - name: Determine scrapers to test
      id: scrapers
      run: |
        if [ -n "${{ github.event.inputs.scrapers }}" ]; then
          echo "scrapers=${{ github.event.inputs.scrapers }}" >> $GITHUB_OUTPUT
        else
          # Check for changed scraper config files
          CHANGED_CONFIGS=$(git diff --name-only HEAD~1 HEAD -- "src/scrapers/configs/*.yaml" | sed 's|src/scrapers/configs/||' | sed 's|\.yaml||' | tr '\n' ',')
          
          if [ -n "$CHANGED_CONFIGS" ]; then
            # Remove trailing comma
            CHANGED_CONFIGS=$(echo "$CHANGED_CONFIGS" | sed 's/,$//')
            echo "scrapers=$CHANGED_CONFIGS" >> $GITHUB_OUTPUT
            echo "Testing changed scrapers: $CHANGED_CONFIGS"
          else
            echo "scrapers=all" >> $GITHUB_OUTPUT
            echo "No scraper configs changed, testing all scrapers"
          fi
        fi

    - name: Run local tests
      if: steps.scrapers.outputs.scrapers == 'all'
      run: |
        uv run python tests/scraper_testing.py --all --verbose
      env:
        PYTHONPATH: ${{ github.workspace }}

    - name: Run specific scraper tests
      if: steps.scrapers.outputs.scrapers != 'all'
      run: |
        for scraper in $(echo "${{ github.event.inputs.scrapers }}" | tr ',' '\n'); do
          echo "Testing scraper: $scraper"
          uv run python tests/scraper_testing.py --scraper "$scraper" --verbose
        done
      env:
        PYTHONPATH: ${{ github.workspace }}

    - name: Generate test report
      if: always()
      run: |
        echo "## Local Testing Report" > test_report.md
        echo "- **Date:** $(date)" >> test_report.md
        echo "- **Mode:** local" >> test_report.md
        echo "- **Scrapers:** ${{ steps.scrapers.outputs.scrapers }}" >> test_report.md
        echo "- **Status:** ${{ job.status }}" >> test_report.md
        echo "" >> test_report.md
        echo "### Test Results" >> test_report.md
        echo "\`\`\`" >> test_report.md
        cat test_output.log >> test_report.md 2>/dev/null || echo "No test output available" >> test_report.md
        echo "\`\`\`" >> test_report.md

    - name: Upload test report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: local-test-report
        path: test_report.md

    - name: Notify on failure
      if: failure()
      run: |
        echo "âŒ Local testing failed!"
        # Add notification logic here (Slack, email, etc.)

  # Job to analyze data quality of scraper outputs and archive results
  data-quality-check:
    needs: local-test
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install Chrome
      run: |
        sudo apt-get update
        sudo apt-get install -y wget gnupg ca-certificates fonts-liberation libappindicator3-1 libasound2 libatk-bridge2.0-0 libatk1.0-0 libatspi2.0-0 libcups2 libdbus-1-3 libdrm2 libgdk-pixbuf2.0-0 libgtk-3-0 libnspr4 libnss3 libx11-xcb1 libxcomposite1 libxdamage1 libxrandr2 xdg-utils
        wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list'
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable

    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH

    - name: Create virtual environment
      run: |
        uv venv

    - name: Install dependencies
      run: |
        uv pip install -e .

    - name: Run data quality analysis
      run: |
        uv run python -c "
        import sys
        from pathlib import Path
        
        # Add project root to path
        PROJECT_ROOT = Path('.')
        sys.path.insert(0, str(PROJECT_ROOT))
        
        from tests.fixtures.scraper_validator import ScraperValidator
        from src.scrapers.parser.yaml_parser import ScraperConfigParser
        
        validator = ScraperValidator()
        
        print('ðŸ” Data Quality Analysis')
        print('=' * 50)
        
        # Get all scraper configs
        configs_dir = PROJECT_ROOT / 'src' / 'scrapers' / 'configs'
        parser = ScraperConfigParser()
        
        for config_file in configs_dir.glob('*.yaml'):
            scraper_name = config_file.stem
            print(f'\\nðŸ“Š {scraper_name.upper()}')
            print('-' * 30)
            
            try:
                config = parser.load_from_file(config_file)
                test_skus = config.test_skus or []
                print(f'Test SKUs: {len(test_skus)} configured')
                print(f'Base URL: {config.base_url}')
                print(f'Timeout: {config.timeout}s')
            except Exception as e:
                print(f'Error loading config: {e}')
        
        print('\\nâœ… Data quality check completed')
        "

    - name: Archive test results
      run: |
        mkdir -p test-results-archive
        cp -r results/* test-results-archive/ 2>/dev/null || true
        echo "Test results archived on $(date)" > test-results-archive/archive_info.txt

    - name: Upload test results archive
      uses: actions/upload-artifact@v4
      with:
        name: test-results-archive
        path: test-results-archive/